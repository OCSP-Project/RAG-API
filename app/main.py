import os, json
from typing import List, Dict, Optional, Any
import requests
import psycopg2
from psycopg2.extras import RealDictCursor
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from datetime import datetime
import google.generativeai as genai
from fastapi.responses import StreamingResponse
from docling.document_converter import DocumentConverter
from langchain_text_splitters import RecursiveCharacterTextSplitter
import time, io, requests as _rq

import logging
logger = logging.getLogger("rag-api")
logger.setLevel(logging.INFO)


APP_TITLE = "OCSP RAG API"
APP_VERSION = "1.0.0"

DB_URL = os.getenv("URL") or os.getenv("DATABASE_URL") or ""
COLAB_EMBEDDING_URL = (os.getenv("COLAB_EMBEDDING_URL") or "").rstrip("/")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
EMBED_DIM = int(os.getenv("EMBED_DIM", "768"))  # ‚úÖ Fixed: 768 cho model ti·∫øng Vi·ªát

# C·∫•u h√¨nh Gemini
if GEMINI_API_KEY and GEMINI_API_KEY != "your-gemini-key-here":
    genai.configure(api_key=GEMINI_API_KEY)
    try:
        GEMINI_MODEL = genai.GenerativeModel("gemini-1.5-flash")
    except Exception:
        GEMINI_MODEL = None
else:
    GEMINI_MODEL = None

app = FastAPI(title=APP_TITLE, version=APP_VERSION)

# CORS: n·∫øu d√πng credentials, KH√îNG ƒë∆∞·ª£c d√πng "*"
FRONTEND_ORIGIN = os.getenv("FRONTEND_ORIGIN", "")  # v√≠ d·ª•: https://your-frontend.example.com
allow_origins = ["*"] if not FRONTEND_ORIGIN else [FRONTEND_ORIGIN]
allow_credentials = bool(os.getenv("CORS_ALLOW_CREDENTIALS", "true").lower() == "true")
if allow_credentials and allow_origins == ["*"]:
    # T·ª± ƒë·ªông chuy·ªÉn sang kh√¥ng credentials n·∫øu b·∫°n v·∫´n mu·ªën "*"
    allow_credentials = False

app.add_middleware(
    CORSMiddleware,
    allow_origins=allow_origins,
    allow_credentials=allow_credentials,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===================== Pydantic Models =====================
class DocItem(BaseModel):
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class AddDocsRequest(BaseModel):
    docs: List[DocItem]

class QueryRequest(BaseModel):
    query: str
    k: int = 5

class ChatRequest(BaseModel):
    message: str
    user_id: Optional[str] = None
    top_k: int = 5  # ‚úÖ Fixed: Added missing field

class ChatResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]] = Field(default_factory=list)

class StoreChunkRequest(BaseModel):
    content: str
    embedding: List[float]
    metadata: Dict[str, Any] = Field(default_factory=dict)

class IngestURLReq(BaseModel):
    url: str
    chunk_size: int = 1200
    chunk_overlap: int = 150

# ===================== DB Helpers =====================
def get_db_connection():
    if not DB_URL:
        raise HTTPException(500, "DB URL ch∆∞a c·∫•u h√¨nh (env URL ho·∫∑c DATABASE_URL).")
    try:
        return psycopg2.connect(DB_URL)
    except Exception as e:
        raise HTTPException(500, f"Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c DB: {e}")

def _vec_to_pg(v: List[float]) -> str:
    return "[" + ",".join(f"{float(x):.6f}" for x in v) + "]"

# ===================== Embedding Service =====================
def embed_via_colab(texts: List[str]) -> List[List[float]]:
    if not COLAB_EMBEDDING_URL:
        raise HTTPException(500, "COLAB_EMBEDDING_URL ch∆∞a c·∫•u h√¨nh")
    
    url = f"{COLAB_EMBEDDING_URL}/embed"
    headers = {"Content-Type": "application/json"}
    
    # ‚úÖ Fixed: Simplified payload handling
    payload = {"texts": texts}
    
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        r.raise_for_status()  # Raise exception for HTTP errors
        
        data = r.json()
        vecs = data.get("embeddings") or data.get("vectors") or data.get("data")
        
        # Handle response format variations
        if isinstance(vecs, list) and vecs and isinstance(vecs[0], dict) and "embedding" in vecs[0]:
            vecs = [item["embedding"] for item in vecs]
            
        if not isinstance(vecs, list):
            raise ValueError(f"Invalid response format: {data}")
            
        if len(vecs) != len(texts):
            if len(texts) > 1 and len(vecs) == 1:
                vecs = vecs * len(texts)
            else:
                raise ValueError(f"Vector count ({len(vecs)}) != text count ({len(texts)})")
                
        # Validate dimensions
        for i, v in enumerate(vecs):
            if not isinstance(v, list) or len(v) != EMBED_DIM:
                raise ValueError(f"Wrong vector dimension at {i}: {len(v)} != {EMBED_DIM}")
                
        return vecs
        
    except requests.exceptions.RequestException as e:
        raise HTTPException(500, f"Embedding service connection error: {e}")
    except Exception as e:
        raise HTTPException(500, f"Embedding service error: {e}")

def check_colab_health() -> str:
    if not COLAB_EMBEDDING_URL:
        return "not_configured"
    try:
        r = requests.get(f"{COLAB_EMBEDDING_URL}/health", timeout=10)
        return "connected" if r.status_code == 200 else "error"
    except Exception:
        return "disconnected"

# ===================== Startup =====================
@app.on_event("startup")
def on_startup():
    conn = get_db_connection()
    try:
        cur = conn.cursor()
        try:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
        except Exception as e:
            # Kh√¥ng d·ª´ng app n·∫øu thi·∫øu quy·ªÅn; log warning
            print(f"[startup] WARNING: cannot create extension vector: {e}")
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS document_chunks (
            id BIGSERIAL PRIMARY KEY,
            content TEXT NOT NULL,
            embedding VECTOR({EMBED_DIM}) NOT NULL,
            metadata JSONB DEFAULT '{{}}'::jsonb
        );
        """)
        cur.execute("""
        CREATE INDEX IF NOT EXISTS idx_chunks_embedding_ivfflat
        ON document_chunks
        USING ivfflat (embedding vector_cosine_ops)
        WITH (lists = 100);
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS idx_chunks_metadata ON document_chunks USING GIN (metadata);")
        conn.commit()
    finally:
        conn.close()

# ===================== Health & Root =====================
@app.get("/")
def root():
    return {"message": f"{APP_TITLE} is running!", "version": APP_VERSION, "docs": "/docs"}

@app.get("/health")
def health_check():
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM document_chunks;")
        doc_count = cur.fetchone()[0]
        cur.close()
        conn.close()
        db_status = "connected"
    except Exception as e:
        doc_count = 0
        db_status = f"error: {e}"
    return {
        "status": "healthy" if db_status == "connected" else "degraded",
        "database": db_status,
        "document_count": doc_count,
        "colab_embedding_service": check_colab_health(),
        "gemini_configured": GEMINI_MODEL is not None,
        "embed_dim": EMBED_DIM,
        "timestamp": datetime.now().isoformat()
    }

# ===================== Core Endpoints =====================
@app.post("/add_docs")
def add_docs(req: AddDocsRequest):
    if not req.docs:
        raise HTTPException(400, "docs r·ªóng")
    texts = [d.content for d in req.docs]
    vecs = embed_via_colab(texts)
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        ids = []
        for d, v in zip(req.docs, vecs):
            v_str = _vec_to_pg(v)
            cur.execute("""
                INSERT INTO document_chunks (content, embedding, metadata)
                VALUES (%s, %s::vector, %s)
                RETURNING id
            """, (d.content, v_str, json.dumps(d.metadata)))
            ids.append(cur.fetchone()[0])
        conn.commit()
        return {"added": len(ids), "ids": ids}
    except Exception as e:
        conn.rollback()
        raise HTTPException(500, str(e))
    finally:
        cur.close()
        conn.close()

@app.post("/store-chunk")
def store_chunk(request: StoreChunkRequest):
    if len(request.embedding) != EMBED_DIM:
        raise HTTPException(400, f"Embedding ph·∫£i c√≥ {EMBED_DIM} chi·ªÅu.")
    v_str = _vec_to_pg(request.embedding)
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT INTO document_chunks (content, embedding, metadata)
            VALUES (%s, %s::vector, %s)
            RETURNING id
        """, (request.content, v_str, json.dumps(request.metadata)))
        cid = cur.fetchone()[0]
        conn.commit()
        return {"message": "Chunk stored successfully", "id": cid}
    except Exception as e:
        conn.rollback()
        raise HTTPException(500, str(e))
    finally:
        cur.close()
        conn.close()

@app.post("/search-similar")
def search_similar(req: QueryRequest):
    q_vec = embed_via_colab([req.query])[0]
    v_str = _vec_to_pg(q_vec)
    conn = get_db_connection()
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute("""
            SELECT id, content, metadata, 1 - (embedding <=> %s::vector) AS score
            FROM document_chunks
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (v_str, v_str, req.k))
        rows = cur.fetchall()
        return {"query": req.query, "results": [
            {"id": r["id"], "content": r["content"], "metadata": r["metadata"], "score": float(r["score"])}
            for r in rows
        ]}
    finally:
        cur.close()
        conn.close()

@app.post("/rag")
def rag(req: QueryRequest):
    try:
        res = search_similar(req)
        hits = res["results"]
        ctx = "\n\n---\n\n".join(f"[{i+1}] {h['content']}" for i, h in enumerate(hits)) or "N/A"
        if not GEMINI_MODEL:
            raise HTTPException(500, "Gemini API ch∆∞a c·∫•u h√¨nh")
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ x√¢y d·ª±ng t·∫°i Vi·ªát Nam.

C√¢u h·ªèi: {req.query}

Ng·ªØ c·∫£nh:
{ctx}

Y√™u c·∫ßu:
- Tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh. N·∫øu thi·∫øu, n√≥i r√µ l√† ch∆∞a ƒë·ªß d·ªØ li·ªáu.
- D√πng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn.
- Tr√≠ch d·∫´n [1], [2]... ·ª©ng v·ªõi th·ª© t·ª± ng·ªØ c·∫£nh khi c√≥ th·ªÉ.
""".strip()
        try:
            resp = GEMINI_MODEL.generate_content(prompt)
            answer = getattr(resp, "text", None) or (resp.candidates[0].content.parts[0].text if getattr(resp, "candidates", None) else "")
            if not answer:
                answer = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ ng·ªØ c·∫£nh hi·ªán c√≥."
        except Exception as e:
            raise HTTPException(500, f"L·ªói model: {e}")
        return {"answer": answer, "retrieved": hits}
    except Exception as e:
        raise HTTPException(500, f"RAG error: {e}")

# ===== Document Upload & Processing =====
class DocumentUploadRequest(BaseModel):
    source: str  # URL ho·∫∑c file path
    chunk_size: int = 500
    chunk_overlap: int = 50

@app.post("/upload_document")
async def upload_document(req: DocumentUploadRequest):
    """Upload v√† parse document t·ª´ URL b·∫±ng IBM Docling"""
    try:
        
        try:
            from docling.document_converter import DocumentConverter
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            import time
        except ImportError:
            raise HTTPException(500, "Docling ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. C·∫ßn c√†i docling v√† langchain-text-splitters")

        # Convert document
        converter = DocumentConverter()
        start_time = time.time()
        result = converter.convert(req.source)
        end_time = time.time()

        # Export to markdown
        markdown_content = result.document.export_to_markdown()

        # Split into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=req.chunk_size,
            chunk_overlap=req.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_text(markdown_content)

        # Create docs for embedding
        docs = []
        for i, chunk in enumerate(chunks):
            docs.append({
                "content": chunk.strip(),
                "metadata": {
                    "type": "document",
                    "source": req.source,
                    "chunk_index": i,
                    "processing_time": end_time - start_time,
                    "timestamp": datetime.now().isoformat()
                }
            })

        # Embed v√† store chunks
        if docs:
            texts = [d["content"] for d in docs]
            vecs = embed_via_colab(texts)

            conn = get_db_connection()
            cur = conn.cursor()
            try:
                ids = []
                for d, v in zip(docs, vecs):
                    v_str = _vec_to_pg(v)
                    cur.execute(
                        """
                        INSERT INTO document_chunks (content, embedding, metadata)
                        VALUES (%s, %s::vector, %s)
                        RETURNING id
                        """,
                        (d["content"], v_str, json.dumps(d["metadata"]))
                    )
                    ids.append(cur.fetchone()[0])
                conn.commit()

                return {
                    "message": "Document processed successfully",
                    "source": req.source,
                    "processing_time": end_time - start_time,
                    "chunks_created": len(chunks),
                    "chunks_stored": len(ids),
                    "chunk_ids": ids
                }
            except Exception as e:
                conn.rollback()
                raise HTTPException(500, f"Database error: {str(e)}")
            finally:
                cur.close()
                conn.close()
        else:
            return {"message": "No content to process", "source": req.source}

    except Exception as e:
        raise HTTPException(500, f"Document processing error: {str(e)}")

@app.post("/ingest/url")
def ingest_url(req: IngestURLReq):
    # Fix: Pass URL directly to Docling, don't download first
    conv = DocumentConverter()
    try:
        result = conv.convert(req.url)  # Pass URL directly
        md = result.document.export_to_markdown()
    except Exception as e:
        raise HTTPException(500, f"Document conversion failed: {e}")

    # Rest of code stays same
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=req.chunk_size, chunk_overlap=req.chunk_overlap,
        separators=["\n\n","\n"," ",""]
    )
    chunks = splitter.split_text(md)

    vecs = embed_via_colab(chunks)
    conn = get_db_connection(); cur = conn.cursor()
    try:
        for c,v in zip(chunks, vecs):
            cur.execute("""
              INSERT INTO document_chunks(content, embedding, metadata)
              VALUES (%s, %s::vector, %s)
            """, (c, _vec_to_pg(v), json.dumps({"source": req.url})))
        conn.commit()
        return {"ok": True, "chunks": len(chunks)}
    finally:
        cur.close(); conn.close()

# ===================== Chat (RAG wrapper) =====================
@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    logger.info(f"Chat request received: {req.message}")
    
    try:
        # Check if database has documents first
        conn = get_db_connection()
        cur = conn.cursor()
        try:
            cur.execute("SELECT COUNT(*) FROM document_chunks;")
            doc_count = cur.fetchone()[0]
            
            if doc_count == 0:
                return ChatResponse(
                    response="Ch√†o b·∫°n! Hi·ªán t·∫°i h·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu v·ªÅ c√°c nh√† th·∫ßu. B·∫°n c√≥ th·ªÉ upload t√†i li·ªáu ho·∫∑c li√™n h·ªá admin ƒë·ªÉ c·∫≠p nh·∫≠t th√¥ng tin nh√©! üòä",
                    sources=[]
                )
        finally:
            cur.close()
            conn.close()
        
        # Search for similar documents with lower threshold
        search_req = QueryRequest(query=req.message, k=req.top_k)
        search_result = search_similar(search_req)
        hits = search_result["results"]
        
        # Build context with more details
        if hits:
            ctx_parts = []
            for i, h in enumerate(hits, 1):
                content = h['content'].strip()
                metadata = h.get('metadata', {})
                source_info = f" (Ngu·ªìn: {metadata.get('source', 'database')})" if metadata.get('source') else ""
                ctx_parts.append(f"[{i}] {content}{source_info}")
            context = "\n\n".join(ctx_parts)
        else:
            context = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."

        # Enhanced friendly consultant prompt
        prompt = f"""
B·∫°n l√† Minh - m·ªôt t∆∞ v·∫•n vi√™n chuy√™n nghi·ªáp v√† th√¢n thi·ªán v·ªÅ x√¢y d·ª±ng t·∫°i ƒê√† N·∫µng. 

PHONG C√ÅCH TRAO ƒê·ªîI:
- G·ªçi kh√°ch h√†ng l√† "anh/ch·ªã" m·ªôt c√°ch l·ªãch s·ª±
- Nhi·ªát t√¨nh, chu ƒë√°o nh∆∞ nh√¢n vi√™n t∆∞ v·∫•n th·ª±c t·∫ø  
- Khi thi·∫øu th√¥ng tin: h·ªèi th√™m chi ti·∫øt ƒë·ªÉ t∆∞ v·∫•n ch√≠nh x√°c h∆°n
- Lu√¥n ƒë∆∞a ra g·ª£i √Ω c·ª• th·ªÉ v√† thi·∫øt th·ª±c
- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi m·ªü ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£

QUY T·∫ÆC TR·∫¢ L·ªúI:
- D·ª±a v√†o th√¥ng tin trong [NG·ªÆ C·∫¢NH] ƒë·ªÉ t∆∞ v·∫•n
- N·∫øu thi·∫øu th√¥ng tin: "ƒê·ªÉ t∆∞ v·∫•n ch√≠nh x√°c h∆°n, anh/ch·ªã c√≥ th·ªÉ cho m√¨nh bi·∫øt th√™m..."
- Lu√¥n tr√≠ch d·∫´n ngu·ªìn [1], [2] khi c√≥ th√¥ng tin c·ª• th·ªÉ
- ƒê·ªÅ xu·∫•t 2-3 l·ª±a ch·ªçn ph√π h·ª£p nh·∫•t v·ªõi y√™u c·∫ßu

[NG·ªÆ C·∫¢NH]
{context}

[C√ÇU H·ªéI KH√ÅCH H√ÄNG]
{req.message}

H√£y tr·∫£ l·ªùi nh∆∞ m·ªôt t∆∞ v·∫•n vi√™n chuy√™n nghi·ªáp:
        """.strip()

        if not GEMINI_MODEL:
            raise HTTPException(500, "Gemini API ch∆∞a c·∫•u h√¨nh")
        
        try:
            out = GEMINI_MODEL.generate_content(prompt)
            answer = out.text
            
            # Add fallback if still generic response
            if "ch∆∞a ƒë·ªß d·ªØ li·ªáu" in answer.lower() and hits:
                answer = f"""
Ch√†o anh/ch·ªã! M√¨nh ƒë√£ t√¨m th·∫•y m·ªôt s·ªë th√¥ng tin li√™n quan trong h·ªá th·ªëng. 

{answer}

ƒê·ªÉ m√¨nh c√≥ th·ªÉ t∆∞ v·∫•n ch√≠nh x√°c h∆°n, anh/ch·ªã c√≥ th·ªÉ chia s·∫ª th√™m:
- Lo·∫°i c√¥ng tr√¨nh mu·ªën x√¢y (nh√† ph·ªë, bi·ªát th·ª±, cao ·ªëc...)
- Khu v·ª±c c·ª• th·ªÉ t·∫°i ƒê√† N·∫µng
- Th·ªùi gian d·ª± ki·∫øn kh·ªüi c√¥ng

M√¨nh s·∫Ω gi√∫p anh/ch·ªã t√¨m nh√† th·∫ßu ph√π h·ª£p nh·∫•t! üòä
                """.strip()
                
        except Exception as e:
            logger.error(f"Gemini error: {e}")
            answer = "Xin l·ªói anh/ch·ªã, h·ªá th·ªëng ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c. Anh/ch·ªã vui l√≤ng th·ª≠ l·∫°i sau m·ªôt ch√∫t nh√©! üôè"
            
        return ChatResponse(
            response=answer,
            sources=[
                {
                    "id": int(h["id"]), 
                    "score": float(h["score"]), 
                    "source": (h.get("metadata") or {}).get("source", "database")
                }
                for h in hits
            ],
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}")
        return ChatResponse(
            response="Xin l·ªói anh/ch·ªã, m√¨nh ƒëang g·∫∑p ch√∫t v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t. Anh/ch·ªã th·ª≠ l·∫°i sau nh√©! üòä",
            sources=[]
        )
    


@app.post("/debug/search-detailed")
def debug_search_detailed(req: QueryRequest):
    """Debug search with detailed scoring"""
    q_vec = embed_via_colab([req.query])[0]
    v_str = _vec_to_pg(q_vec)
    conn = get_db_connection()
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute("""
            SELECT id, content, metadata, 
                   1 - (embedding <=> %s::vector) AS score,
                   embedding <=> %s::vector AS distance
            FROM document_chunks
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (v_str, v_str, v_str, req.k))
        rows = cur.fetchall()
        return {
            "query": req.query, 
            "results": [
                {
                    "id": r["id"], 
                    "content": r["content"][:200] + "..." if len(r["content"]) > 200 else r["content"],
                    "metadata": r["metadata"], 
                    "score": float(r["score"]),
                    "distance": float(r["distance"])
                }
                for r in rows
            ]
        }
    finally:
        cur.close()
        conn.close()    

# ===================== SSE Streaming Chat =====================
def _build_rag_prompt(question: str, hits: list) -> str:
    ctx_blocks = []
    for i, h in enumerate(hits, 1):
        md = h.get("metadata") or {}
        src = md.get("source") or md.get("url") or md.get("path")
        snippet = h.get("content") or ""
        if len(snippet) > 1200:
            snippet = snippet[:1200] + "..."
        ctx_blocks.append(f"[{i}] (source: {src or 'N/A'})\n{snippet}")
    context = "\n\n".join(ctx_blocks) if ctx_blocks else "‚Äî"
    return f"""B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t x√¢y d·ª±ng, tr·∫£ l·ªùi ng·∫Øn g·ªçn, b√°m s√°t ng·ªØ c·∫£nh.
N·∫øu thi·∫øu th√¥ng tin th√¨ n√≥i 'm√¨nh kh√¥ng ch·∫Øc t·ª´ t√†i li·ªáu'.
Cu·ªëi c√¢u tr·∫£ l·ªùi n√™n li·ªát k√™ ngu·ªìn theo d·∫°ng [1], [2] t∆∞∆°ng ·ª©ng ƒëo·∫°n ƒë√£ d√πng.

[CNTX]
{context}

[C√ÇU H·ªéI]
{question}
"""

@app.get("/chat/stream")
def chat_stream(q: str, k: int = 5):
    """Server-Sent Events: stream c√¢u tr·∫£ l·ªùi LLM theo th·ªùi gian th·ª±c"""
    if not GEMINI_MODEL:
        raise HTTPException(500, "GEMINI_API_KEY ch∆∞a c·∫•u h√¨nh")

    # 1) Embed + retrieve
    q_vec = embed_via_colab([q])[0]
    v_str = _vec_to_pg(q_vec)
    conn = get_db_connection()
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute("""
            SELECT id, content, metadata, 1 - (embedding <=> %s::vector) AS score
            FROM document_chunks
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (v_str, v_str, k))
        hits = cur.fetchall()
    finally:
        cur.close()
        conn.close()

    prompt = _build_rag_prompt(q, hits)

    def event_gen():
        # G·ª≠i ngu·ªìn tr∆∞·ªõc
        try:
            import json as _json
            yield "event: sources\n"
            yield "data: " + _json.dumps([
                {"id": int(h["id"]), "score": float(h["score"]), "source": (h.get("metadata") or {}).get("source")}
                for h in hits
            ]) + "\n\n"
        except Exception:
            # Kh√¥ng ch·∫∑n stream n·∫øu l·ªói serialize ngu·ªìn
            pass

        # 2) Stream t·ª´ Gemini
        try:
            for chunk in GEMINI_MODEL.generate_content(prompt, stream=True):
                txt = getattr(chunk, "text", "") or ""
                if txt:
                    # M·ªói message l√† m·ªôt 'data: ...\n\n' theo SSE
                    yield "data: " + txt + "\n\n"
        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

        yield "data: [DONE]\n\n"

    return StreamingResponse(event_gen(), media_type="text/event-stream")